"""
Gemini LLM Client for Data Formulator Integration
Provides natural language data transformation capabilities using both structured parsing and pandas DataFrame agent
"""
import os
import json
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
import re
import google.generativeai as genai

# Pandas DataFrame Agent imports - temporarily disabled due to compatibility issues
try:
    # from langchain.agents.agent_types import AgentType
    # from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
    # from langchain_google_genai import GoogleGenerativeAI
    LANGCHAIN_AVAILABLE = False  # Temporarily disabled
except ImportError as e:
    print(f"LangChain imports failed: {e}")
    LANGCHAIN_AVAILABLE = False


class GeminiDataFormulator:
    """
    Gemini Data Formulator - AI-Powered Data Analysis Engine
    
    Main class that provides natural language data transformation capabilities using Google's Gemini LLM.
    Generates and executes pandas code to answer data questions in plain English.
    
    Features:
        - Natural language to pandas code generation
        - Safe code execution in sandboxed environment
        - Token usage tracking for cost estimation
        - Statistical summary generation
        - Metric calculation from text queries
    
    Architecture:
        1. User query ‚Üí LLM generates pandas code
        2. Code validation and cleaning
        3. Safe execution on real dataset
        4. Result formatting and presentation
    """
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2.0-flash-exp"):
        """
        Initialize Gemini Data Formulator
        
        Args:
            api_key: Google Gemini API key (required)
            model: Gemini model name (default: "gemini-2.0-flash-exp")
        
        Sets up:
            - Gemini API configuration
            - Model instance
            - LangChain pandas agent (if available)
        """
        if not api_key:
            raise ValueError("API key is required. Please provide a valid Gemini API key.")
        
        self.api_key = api_key
        self.model_name = model
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)
        
        # Initialize pandas DataFrame agent if langchain is available
        self.pandas_agent = None
        if LANGCHAIN_AVAILABLE:
            try:
                # Map frontend model names to LangChain compatible names
                langchain_model = self._get_langchain_model_name(model)
                self.llm = GoogleGenerativeAI(
                    model=langchain_model,
                    google_api_key=self.api_key,
                    temperature=0.1
                )
                print(f"‚úÖ Pandas DataFrame Agent initialized successfully with {langchain_model}")
            except Exception as e:
                print(f"‚ùå Failed to initialize pandas DataFrame agent: {e}")
                self.llm = None
        else:
            print("‚ö†Ô∏è  LangChain not available, using structured parsing only")
    
    def _get_langchain_model_name(self, model: str) -> str:
        """
        Model Name Mapper
        Converts frontend model names to LangChain-compatible model identifiers.
        
        Args:
            model: Frontend model name (e.g., "gemini-2.0-flash-exp")
        
        Returns:
            LangChain-compatible model name
        """
        model_mapping = {
            "gemini-1.5-flash": "gemini-1.5-flash",
            "gemini-2.0-flash": "gemini-2.0-flash", 
            "gemini-2.0-flash-exp": "gemini-2.0-flash"  # fallback to stable version for LangChain
        }
        return model_mapping.get(model, "gemini-2.0-flash")
    
    def run_gemini(self, prompt: str, model: str = "gemini-2.0-flash-exp") -> str:
        """
        Basic Gemini API Call
        Sends a prompt to Gemini LLM and returns the text response.
        
        Args:
            prompt: Natural language prompt/question
            model: Model name (currently not used, instance model is used)
        
        Returns:
            str: Generated response text
        
        Raises:
            Exception: For authentication errors or API connection issues
        
        Note: For token tracking, use run_gemini_with_usage() instead
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            error_msg = str(e).lower()
            # Check for authentication/API key errors
            if any(keyword in error_msg for keyword in ['api key', 'api_key', 'authentication', 'unauthorized', '401', '403', 'permission', 'invalid key']):
                print(f"‚ùå Gemini API authentication error: {e}")
                raise Exception(f"Invalid API key or authentication failed: {str(e)}")
            # Check for network/connection errors
            elif any(keyword in error_msg for keyword in ['connection', 'network', 'timeout', 'unreachable']):
                print(f"‚ùå Gemini API connection error: {e}")
                raise Exception(f"Unable to connect to Gemini API: {str(e)}")
            else:
                # For other errors, log and re-raise
                print(f"‚ùå Gemini API error: {e}")
                raise Exception(f"Gemini API error: {str(e)}")
    
    def run_gemini_with_usage(self, prompt: str, model: str = "gemini-2.0-flash-exp") -> tuple[str, dict]:
        """
        Gemini API Call with Token Tracking
        Sends a prompt to Gemini and returns both the response and token usage metrics.
        Essential for cost estimation and tracking.
        
        Args:
            prompt: Natural language prompt/question
            model: Model name (currently not used, instance model is used)
        
        Returns:
            tuple: (response_text, token_usage_dict)
            - response_text: Generated text
            - token_usage_dict: {"inputTokens": int, "outputTokens": int, "totalTokens": int}
        
        Token Estimation:
            - Tries to extract from response.usage_metadata
            - Falls back to word-count estimation if unavailable
        
        Raises:
            Exception: For authentication errors or API connection issues
        """
        try:
            response = self.model.generate_content(prompt)
            
            # Extract token usage from response
            token_usage = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                token_usage = {
                    "inputTokens": getattr(response.usage_metadata, 'prompt_token_count', 0),
                    "outputTokens": getattr(response.usage_metadata, 'candidates_token_count', 0),
                    "totalTokens": getattr(response.usage_metadata, 'total_token_count', 0)
                }
            else:
                # Fallback: estimate tokens (rough approximation)
                estimated_input = len(prompt.split()) * 1.3  # rough token estimation
                estimated_output = len(response.text.split()) * 1.3 if response.text else 0
                token_usage = {
                    "inputTokens": int(estimated_input),
                    "outputTokens": int(estimated_output),
                    "totalTokens": int(estimated_input + estimated_output)
                }
            
            return response.text, token_usage
        except Exception as e:
            error_msg = str(e).lower()
            # Check for authentication/API key errors
            if any(keyword in error_msg for keyword in ['api key', 'api_key', 'authentication', 'unauthorized', '401', '403', 'permission', 'invalid key']):
                print(f"‚ùå Gemini API authentication error: {e}")
                raise Exception(f"Invalid API key or authentication failed: {str(e)}")
            # Check for network/connection errors
            elif any(keyword in error_msg for keyword in ['connection', 'network', 'timeout', 'unreachable']):
                print(f"‚ùå Gemini API connection error: {e}")
                raise Exception(f"Unable to connect to Gemini API: {str(e)}")
            else:
                # For other errors, log and re-raise to avoid silent failures
                print(f"‚ùå Gemini API error: {e}")
                raise Exception(f"Gemini API error: {str(e)}")
    
    def _simulate_gemini_response(self, prompt: str) -> str:
        """
        Fallback Response Simulator
        Simulates Gemini responses when API is unavailable or fails.
        Uses pattern matching to detect common data transformation queries.
        
        Args:
            prompt: User's natural language query
        
        Returns:
            str: Simulated response based on query patterns
        
        Supported Patterns:
            - Filter operations (e.g., "filter where column = value")
            - Group by operations
            - Aggregations (sum, average, count)
        """
        query = prompt.lower()
        
        # Pattern matching for common transformations
        if "filter" in query and "=" in query:
            # Extract filter condition
            filter_match = re.search(r'filter.*?(\w+)\s*=\s*[\'"]?(\w+)[\'"]?', query, re.IGNORECASE)
            if filter_match:
                column, value = filter_match.groups()
                return f"FILTER: {column} == '{value}'"
        
        if "group by" in query:
            # Extract group by column
            group_match = re.search(r'group by\s+(\w+)', query, re.IGNORECASE)
            if group_match:
                column = group_match.group(1)
                return f"GROUP_BY: {column}"
        
        if "sum" in query:
            return "AGGREGATE: sum"
        elif "average" in query or "mean" in query:
            return "AGGREGATE: mean"
        elif "count" in query:
            return "AGGREGATE: count"
        
        # Default response
        return "TRANSFORM: Basic data analysis"
    
    def test_configuration(self) -> Dict[str, Any]:
        """
        Configuration Tester
        Tests the Gemini API key and model configuration with a simple query.
        Used by the /test-config endpoint to verify credentials before use.
        
        Returns:
            Dict containing:
                - success: bool
                - message: Status message
                - model: Model name
                - api_key_configured: bool
                - langchain_available: bool
                - test_response: Sample response text
                - token_usage: Token metrics from test
        """
        try:
            # Test with a simple prompt
            test_prompt = "Hello, this is a test. Please respond with 'Configuration test successful'."
            response, token_usage = self.run_gemini_with_usage(test_prompt)
            
            return {
                "success": True,
                "message": "Configuration test successful",
                "model": self.model_name,
                "api_key_configured": bool(self.api_key),
                "langchain_available": LANGCHAIN_AVAILABLE,
                "test_response": response[:100] + "..." if len(response) > 100 else response,
                "token_usage": token_usage
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": self.model_name,
                "api_key_configured": bool(self.api_key),
                "langchain_available": LANGCHAIN_AVAILABLE,
                "token_usage": {"inputTokens": 0, "outputTokens": 0, "totalTokens": 0}
            }
    
    def get_text_analysis(self, user_query: str, dataset: pd.DataFrame, dataset_id: Optional[str] = None, dataset_metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Main AI Data Analysis Pipeline
        Primary entry point for AI-powered data exploration.
        Orchestrates the complete flow from query to answer.
        
        Process Flow:
            1. Analyze dataset structure (columns, types, samples)
            2. Generate pandas code using Gemini
            3. Execute code safely on real dataset
            4. Format and return results with token usage
        
        Args:
            user_query: Natural language question (e.g., "Which state has the highest revenue?")
            dataset: Pandas DataFrame to analyze
            dataset_id: Optional dataset ID (for logging purposes)
            dataset_metadata: Optional pre-loaded dataset analysis metadata for enhanced context
        
        Returns:
            Dict containing:
                - success: bool
                - answer: Text answer to the query
                - query: Original user query
                - dataset_info: Dataset metadata
                - code_steps: List of generated pandas code
                - reasoning_steps: Execution steps
                - tabular_data: Extracted table data (if any)
                - has_table: bool indicating if response contains tabular data
                - token_usage: Token metrics
        
        Used by: /ai-explore endpoint
        """
        try:
            print(f"ü§ñ AI Analysis for: '{user_query}'")
            print(f"üìä Dataset: {dataset.shape[0]} rows, {dataset.shape[1]} columns")
            print(f"üîç Columns: {list(dataset.columns)}")
            
            # Log dataset context usage
            if dataset_metadata and dataset_metadata.get('success'):
                summary_preview = dataset_metadata.get('dataset_summary', '')[:100] + '...' if len(dataset_metadata.get('dataset_summary', '')) > 100 else dataset_metadata.get('dataset_summary', '')
                print(f"üìã Using enhanced dataset context: {summary_preview}")
                print(f"üìä Enhanced column descriptions: {len(dataset_metadata.get('columns', []))} columns")
            elif dataset_id:
                print(f"üìã Dataset ID provided ({dataset_id[:8]}...) but no metadata available - using basic analysis")
            else:
                print("üìã No enhanced context available - using basic dataset analysis")
            
            # Generate Python code using Gemini with enhanced context
            code, token_usage = self._generate_pandas_code(user_query, dataset, dataset_metadata)
            
            if not code:
                return {
                    "success": False,
                    "answer": "Failed to generate analysis code",
                    "query": user_query,
                    "dataset_info": f"Dataset: {dataset.shape[0]} rows, {dataset.shape[1]} columns",
                    "code_steps": [],
                    "reasoning_steps": [],
                    "tabular_data": [],
                    "has_table": False,
                    "token_usage": token_usage
                }
            
            # Execute the generated code
            result = self._execute_pandas_code(code, dataset, user_query)
            result["token_usage"] = token_usage
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error in get_text_analysis: {str(e)}")
            return {
                "success": False,
                "answer": f"I encountered an error while processing your query: {str(e)}",
                "query": user_query,
                "dataset_info": f"Dataset: {dataset.shape[0]} rows, {dataset.shape[1]} columns",
                "code_steps": [],
                "reasoning_steps": [],
                "tabular_data": [],
                "has_table": False,
                "token_usage": {"inputTokens": 0, "outputTokens": 0, "totalTokens": 0}
            }
    
    def _analyze_dataset_structure(self, dataset: pd.DataFrame) -> Dict[str, Any]:
        """
        Dataset Structure Analyzer
        Analyzes the DataFrame structure and generates context for code generation.
        Creates generic code examples based on actual column types.
        
        Args:
            dataset: Pandas DataFrame to analyze
        
        Returns:
            Dict containing:
                - numeric_columns: List of numeric column names
                - categorical_columns: List of categorical column names
                - generic_example: Sample pandas code using actual columns
                - sample_data: First 3 rows as string
        
        Purpose:
            Provides Gemini with concrete examples using the actual dataset structure,
            helping it generate more accurate and executable code.
        """
        numeric_columns = dataset.select_dtypes(include=[np.number]).columns.tolist()
        categorical_columns = dataset.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # Create generic example based on dataset structure
        if len(numeric_columns) > 0 and len(categorical_columns) > 0:
            example_numeric = numeric_columns[0]
            example_categorical = categorical_columns[0]
            generic_example = f"""# Answer the user's query using df
result = df.nlargest(5, '{example_numeric}')[['{example_categorical}', '{example_numeric}']]
print("Top 5 records by {example_numeric}:")
print(result.to_string(index=False))"""
        elif len(numeric_columns) > 0:
            example_numeric = numeric_columns[0]
            generic_example = f"""# Answer the user's query using df
result = df.nlargest(5, '{example_numeric}')
print("Top 5 records by {example_numeric}:")
print(result.to_string(index=False))"""
        else:
            first_col = dataset.columns[0] if len(dataset.columns) > 0 else 'column'
            generic_example = f"""# Answer the user's query using df
result = df['{first_col}'].value_counts().head(5)
print("Top 5 most frequent values in {first_col}:")
print(result)"""
        
        return {
            "numeric_columns": numeric_columns,
            "categorical_columns": categorical_columns,
            "generic_example": generic_example,
            "sample_data": dataset.head(3).to_string(index=False)
        }
    
    def _generate_pandas_code(self, user_query: str, dataset: pd.DataFrame, dataset_metadata: Optional[Dict[str, Any]] = None) -> tuple[str, dict]:
        """
        Pandas Code Generator
        Uses Gemini LLM to generate executable pandas code from natural language queries.
        Provides dataset context to ensure generated code matches actual data structure.
        
        Args:
            user_query: Natural language question about the data
            dataset: The actual DataFrame to analyze
            dataset_metadata: Optional AI-generated dataset analysis with semantic context
        
        Returns:
            tuple: (generated_code, token_usage)
                - generated_code: Clean Python pandas code ready for execution
                - token_usage: Token metrics from LLM call
        
        Prompt Engineering:
            - Provides full dataset context (shape, columns, dtypes, samples)
            - Gives concrete examples using actual column names
            - Enforces constraints (no DataFrame creation, only use 'df' variable)
            - Requests print statements for clear output
        
        Code Cleaning:
            - Removes markdown formatting (```python ... ```)
            - Extracts only executable Python code
        """
        try:
            print("ü§ñ Generating pandas code for real dataset...")
            
            # Analyze dataset structure
            dataset_info = self._analyze_dataset_structure(dataset)
            
            # Build enhanced context from metadata if available
            enhanced_context = ""
            if dataset_metadata and dataset_metadata.get('success'):
                dataset_summary = dataset_metadata.get('dataset_summary', '')
                columns_with_descriptions = dataset_metadata.get('columns', [])
                
                if dataset_summary and dataset_summary.strip():
                    enhanced_context += f"\nDATASET CONTEXT & BUSINESS MEANING:\n- Purpose: {dataset_summary.strip()}\n"
                
                if columns_with_descriptions and len(columns_with_descriptions) > 0:
                    enhanced_context += "\nENHANCED COLUMN CONTEXT:\n"
                    column_desc_map = {col.get('name'): col.get('description', '') for col in columns_with_descriptions if col.get('name') and col.get('description')}
                    
                    for col in dataset.columns:
                        col_desc = column_desc_map.get(col, 'Standard data column')
                        if col_desc and col_desc.strip():
                            enhanced_context += f"- {col}: {dict(dataset.dtypes.astype(str))[col]} | {col_desc.strip()}\n"
                        else:
                            enhanced_context += f"- {col}: {dict(dataset.dtypes.astype(str))[col]} | Standard data column\n"
                            
                if not enhanced_context.strip():
                    print("‚ö†Ô∏è Metadata available but empty - falling back to basic analysis")
            
            # Determine context type for logging
            context_type = 'with enhanced semantic context' if (dataset_metadata and dataset_metadata.get('success') and enhanced_context.strip()) else 'with basic structure analysis'
            print(f"üìù Context type: {context_type}")
            
            # Create code generation prompt with enhanced context
            code_generation_prompt = f"""You are a data analyst. Generate Python pandas code to answer the user's query using the provided DataFrame.
{enhanced_context}
REAL DATASET STRUCTURE:
- Variable name: 'df' 
- Shape: {dataset.shape[0]} rows, {dataset.shape[1]} columns
- Columns: {list(dataset.columns)}
- Data types: {dict(dataset.dtypes.astype(str))}
- Sample data (first 3 rows):
{dataset_info['sample_data']}

USER QUERY: "{user_query}"

Generate ONLY Python pandas code that:
1. Uses ONLY the variable 'df' (which contains the real data above)
2. NEVER creates or recreates the DataFrame 
3. Uses appropriate pandas methods based on the business context and column meanings
4. Includes print statements to show results clearly
5. Provides the exact answer to the user's question
6. Works with the actual column names and data types shown above
7. Consider the business/domain context when selecting columns and operations

Example format:
```python
{dataset_info['generic_example']}
```

Generate ONLY the code, no explanations:"""

            # Generate code using Gemini
            code_response_text, token_usage = self.run_gemini_with_usage(code_generation_prompt)
            
            if not code_response_text:
                return "", token_usage
            
            # Extract Python code from response
            generated_code = code_response_text.strip()
            
            # Clean up code (remove markdown formatting)
            if "```python" in generated_code:
                code_lines = generated_code.split("```python")[1].split("```")[0].strip()
            elif "```" in generated_code:
                code_lines = generated_code.split("```")[1].strip()
            else:
                code_lines = generated_code
            
            print("üíª GENERATED PANDAS CODE FOR REAL DATA:")
            print("-" * 50)
            print(code_lines)
            print("-" * 50)
            
            return code_lines, token_usage
                
        except Exception as e:
            print(f"‚ùå Code generation failed: {str(e)}")
            return "", {"inputTokens": 0, "outputTokens": 0, "totalTokens": 0}
    
    def _execute_pandas_code(self, code: str, dataset: pd.DataFrame, user_query: str) -> Dict[str, Any]:
        """
        Safe Code Executor
        Executes generated pandas code in a sandboxed environment on the real dataset.
        Captures output and formats results for user display.
        
        Args:
            code: Generated pandas code to execute
            dataset: The actual DataFrame to analyze
            user_query: Original user query (for context in response)
        
        Returns:
            Dict containing:
                - answer: Formatted text answer with results
                - success: bool indicating execution status
                - reasoning_steps: Execution status messages
                - code_steps: The code that was executed
                - tabular_data: Extracted table rows (if output contains tables)
                - has_table: bool indicating if response has tabular data
        
        Security:
            - Uses exec() in isolated globals namespace
            - Only provides df, pd, numpy, and custom print
            - Captures stdout to prevent console spam
            - No file system or network access
        
        Output Handling:
            - Redirects print to StringIO for capture
            - Attempts to parse tabular output (pipes/formatted tables)
            - Returns human-readable analysis text
        """
        try:
            print("‚ö° EXECUTING CODE ON REAL DATASET...")
            
            # Create safe execution environment
            import io
            from contextlib import redirect_stdout
            
            # Capture output
            captured_output = io.StringIO()
            
            # Create execution globals with REAL dataset
            execution_globals = {
                'df': dataset.copy(),  # ‚úÖ REAL dataset, not fabricated
                'pd': pd,
                'numpy': __import__('numpy'),
                'print': lambda *args, **kwargs: print(*args, **kwargs, file=captured_output)
            }
            
            # Execute generated code on real dataset
            exec(code, execution_globals)
            
            # Get the output
            execution_output = captured_output.getvalue()
            
            print("‚úÖ CODE EXECUTION SUCCESSFUL ON REAL DATA!")
            print("üìã REAL DATA ANALYSIS RESULTS:")
            print("-" * 50)
            print(execution_output)
            print("-" * 50)
            
            # Create analysis text
            analysis_text = f"Based on your real dataset, here are the results for '{user_query}':\n\n{execution_output.strip()}"
            
            # Try to extract tabular data from output
            tabular_data = []
            has_table = False
            
            if execution_output.strip():
                try:
                    # Simple tabular data detection
                    if '|' in execution_output and '\n' in execution_output:
                        lines = execution_output.split('\n')
                        for line in lines:
                            if '|' in line and not line.strip().startswith('|'):
                                tabular_data.append(line.strip())
                        if tabular_data:
                            has_table = True
                            print(f"‚úÖ Successfully parsed tabular data: {len(tabular_data)} lines")
                except Exception as parse_error:
                    print(f"‚ö†Ô∏è Table parsing failed: {parse_error}")
                    has_table = False
            
            return {
                "answer": analysis_text,
                    "success": True,
                "reasoning_steps": ["‚úÖ Executed pandas code on REAL uploaded dataset"],
                "code_steps": [code],  # Show the actual pandas code
                    "tabular_data": tabular_data,
                "has_table": has_table
            }
            
        except Exception as exec_error:
            print(f"‚ùå CODE EXECUTION FAILED: {exec_error}")
            error_msg = f"Error executing pandas code on real dataset: {str(exec_error)}"
            
            return {
                "answer": f"I generated pandas code for your real dataset but encountered an execution error: {error_msg}. The code was: {code}",
                "success": False,
                "reasoning_steps": [f"‚ùå Code execution failed: {str(exec_error)}"],
                "code_steps": [code],
                "tabular_data": [],
                "has_table": False
            }
    
    def calculate_metric(self, user_query: str, dataset_id: str, data: pd.DataFrame) -> Dict[str, Any]:
        """
        AI Metric Calculator
        Calculates specific metrics from natural language descriptions.
        Used by expression nodes to convert text queries into numeric values.
        
        Args:
            user_query: Natural language metric description
                Examples: "average revenue per state", "total population growth"
            dataset_id: ID of the dataset (for logging)
            data: Pandas DataFrame to calculate from
        
        Returns:
            Dict containing:
                - success: bool
                - value: Raw calculated value (numeric)
                - formatted_value: Human-readable formatted string
                - expression: Pandas expression used (e.g., "df['Revenue'].mean()")
                - explanation: Brief description of what was calculated
                - query: Original user query
                - token_usage: Token metrics
        
        Process:
            1. Ask Gemini to provide metric expression + explanation
            2. Parse structured response (METRIC_EXPRESSION, CALCULATED_VALUE, EXPLANATION)
            3. If no value provided, execute expression safely
            4. Format result with thousands separators and decimals
        
        Used by: /ai-calculate-metric endpoint for expression nodes
        """
        try:
            print(f"üßÆ AI Metric calculation started:")
            print(f"   Query: '{user_query}'")
            print(f"   Dataset: {dataset_id}")
            print(f"   Data shape: {data.shape}")
            
            # Create a focused prompt for metric calculation
            prompt = f"""You are a data analyst calculating metrics from a dataset using pandas.

Dataset Information:
- Shape: {data.shape[0]} rows, {data.shape[1]} columns
- Columns: {list(data.columns)}
- Sample data:
{data.head().to_string()}

USER REQUEST: {user_query}

Please provide:
1. The exact pandas expression to calculate this metric
2. The calculated value
3. A brief explanation

Format your response as:
METRIC_EXPRESSION: [pandas expression using 'df' variable]
CALCULATED_VALUE: [the actual numeric result]
EXPLANATION: [brief explanation of what this metric represents]

Use the actual data provided above."""

            response, token_usage = self.run_gemini_with_usage(prompt)
            
            # Parse the response
            metric_expression = ""
            calculated_value = None
            explanation = ""
            
            lines = response.split('\n')
            for line in lines:
                if line.startswith('METRIC_EXPRESSION:'):
                    metric_expression = line.replace('METRIC_EXPRESSION:', '').strip()
                elif line.startswith('CALCULATED_VALUE:'):
                    try:
                        calculated_value = float(line.replace('CALCULATED_VALUE:', '').strip())
                    except:
                        calculated_value = line.replace('CALCULATED_VALUE:', '').strip()
                elif line.startswith('EXPLANATION:'):
                    explanation = line.replace('EXPLANATION:', '').strip()
            
            # If no structured response, try to execute the metric expression
            if metric_expression and calculated_value is None:
                try:
                    # Safe execution environment
                    safe_globals = {'df': data, 'pd': pd, 'np': np, '__builtins__': {}}
                    calculated_value = eval(metric_expression, safe_globals)
                except Exception as e:
                    print(f"‚ùå Failed to execute metric expression: {e}")
                    calculated_value = "Error in calculation"
            
            # Format the value
            formatted_value = calculated_value
            if isinstance(calculated_value, (int, float)):
                if calculated_value == int(calculated_value):
                    formatted_value = f"{int(calculated_value):,}"
                else:
                    formatted_value = f"{calculated_value:,.2f}"
            
                    return {
                "success": True,
                "value": calculated_value,
                "formatted_value": formatted_value,
                "expression": metric_expression,
                "explanation": explanation,
                "query": user_query,
                "token_usage": token_usage
            }
            
        except Exception as e:
            print(f"‚ùå AI Metric calculation failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "value": None,
                "formatted_value": "Error",
                "expression": "",
                "explanation": f"Failed to calculate metric: {str(e)}",
                "query": user_query,
                "token_usage": {"inputTokens": 0, "outputTokens": 0, "totalTokens": 0}
            }